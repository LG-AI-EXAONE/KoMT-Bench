# KoMT-Bench

| ğŸ¤— [**HuggingFace**](https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench) | ğŸ“‘ [**EXAONE 3.0 7.8B Tech Report**](https://arxiv.org/abs/2408.03541) |

<br>

## Introduction

This is an official repository for **KoMT Bench** built by LG AI Research, used to evaluate Korean instruction-following capability of language models, as described in the â€œ[EXAONE 3.0 7.8B Instruction-Tuned Language Model](https://arxiv.org/abs/2408.03541)â€ (Technical Report). KoMT Bench is developed by translating [MT-Bench](https://arxiv.org/abs/2306.05685) [1] dataset into Korean and modifying some questions to reflect the characteristics and cultural nuances of the Korean language. 

All source code in this repository is based on [LMSYSâ€™s FastChat repository](https://github.com/lm-sys/FastChat), and we have adapted it to implement EXAONE 3.0 7.8B model.


<br>
<p>Here are examples from KoMT-Bench:</p>
<br>

<table>
<tr>
	<th>Category</th>
	<th>MT-Bench</th>
	<th>KoMT-Bench</th>
</tr>
<tr height=40>
	<th colspan=3 align="left">Writing</th>
</tr>
<tr>
	<td align="center">1st Turn</td>
	<td>Imagine you are writing a blog post comparing two popular smartphone models. Develop an outline for the blog post, including key points and subheadings to effectively compare and contrast the features, performance, and user experience of the two models. Please answer in fewer than 200 words.</td>
	<td>ë‘ ê°œì˜ ì¸ê¸° ìŠ¤ë§ˆíŠ¸í° ëª¨ë¸ì„ ë¹„êµí•˜ëŠ” ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì„ ì‘ì„±í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ë‘ ëª¨ë¸ì˜ ê¸°ëŠ¥, ì„±ëŠ¥, ì‚¬ìš©ì ê²½í—˜ì„ íš¨ê³¼ì ìœ¼ë¡œ ë¹„êµí•˜ê³  ëŒ€ì¡°í•  ìˆ˜ ìˆë„ë¡ í•µì‹¬ ì‚¬í•­ê³¼ ì†Œì œëª©ì„ í¬í•¨í•˜ì—¬ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì˜ ê°œìš”ë¥¼ ì‘ì„±í•˜ì„¸ìš”. 200ì ì´ë‚´ë¡œ ë‹µí•˜ì‹­ì‹œì˜¤.</td>
</tr>
<tr>
	<td align="center">2nd Turn</td>
	<td>Take your previous response and rephrase it as a limerick.</td>
	<td>ì´ì „ ë‹µë³€ì„ ì¶©ì²­ë„ ì‚¬íˆ¬ë¦¬ë¡œ ì¬ì‘ì„±í•˜ì‹­ì‹œì˜¤.</td>
</tr>

<tr height=40>
	<th colspan=3 align="left">Math</th>
</tr>
<tr>
	<td align="center">1st Turn</td>
	<td>When a number is divided by 10, the remainder is 4. What is the remainder when twice the number is divided by 4?</td>
	<td>ì–´ë–¤ ìˆ«ìë¥¼ 10ìœ¼ë¡œ ë‚˜ëˆˆ ë‚˜ë¨¸ì§€ëŠ” 4ì…ë‹ˆë‹¤. ê·¸ ìˆ«ìì˜ ë‘ ë°°ë¥¼ 4ë¡œ ë‚˜ëˆˆ ë‚˜ë¨¸ì§€ë¥¼ êµ¬í•˜ì„¸ìš”.</td>
</tr>
<tr>
	<td align="center">2nd Turn</td>
	<td>What about when twice the number is divided by 5?</td>
	<td>ê·¸ ìˆ«ìì˜ ë‘ ë°°ë¥¼ 5ë¡œ ë‚˜ëˆ„ë©´ ì–´ë–¨ê¹Œìš”?</td>
</tr>

<tr height=40>
	<th colspan=3 align="left">Humanities</th>
</tr>
<tr>
	<td align="center">1st Turn</td>
	<td>Provide insights into the correlation between economic indicators such as GDP, inflation, and unemployment rates. Explain how fiscal and monetary policies affect those indicators.</td>
	<td>GDP, ì¸í”Œë ˆì´ì…˜, ì‹¤ì—…ë¥ ê³¼ ê°™ì€ ê²½ì œ ì§€í‘œ ê°„ì˜ ìƒê´€ê´€ê³„ì— ëŒ€í•œ í†µì°°ì„ ì œì‹œí•˜ì„¸ìš”. ì´ëŸ¬í•œ ì§€í‘œë“¤ì— ì¬ì • ë° í†µí™” ì •ì±…ì´ ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”.</td>
</tr>
<tr>
	<td align="center">2nd Turn</td>
	<td>Now, explain them again like I'm five.</td>
	<td>ì´ì œ ì œê°€ 5ì‚´ì´ë¼ ìƒê°í•˜ê³  ë‹¤ì‹œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.</td>
</tr>
</table>

<br>

## Setting & Installation

```bash
git clone https://github.com/LG-AI-EXAONE/KoMT-Bench
cd FastChat
bash setting.sh
export OPENAI_API_KEY=<openai key>
export PYTHONPATH=${PYTHONPATH}:${PWD}
```

- Note that we have implemented a *square root* penalty for non-Korean responses, which requires to detect what language each response is written in. To accomplish this, we utilize a language detector developed by Google. **We recommend users check that the detector has been installed correctly in the following path**: `./fastchat/llm_judge/data/mt_bench/model_judgment/language_detector.tflite`

<br>

## Run

```bash
bash run_ko_mt.sh
```

- To evaluate the EXAONE model with a single line of code, just execute the command above.

To run each step separately, please follow the steps below:

### 1. Generating model Answer

```bash
cd fastchat/llm_judge/

# Generating Model Answers
CUDA_VISIBLE_DEVICES=0 python gen_model_answer.py \
		--model-path <model_name or model_path> \
		--model-id <model_id> \
		--dtype bfloat16 
```

- To evaluate the EXAONE model, `<model_name or model_path>` must include the keyword `exaone`.

### 2. Evaluating model Answer

```bash
cd fastchat/llm_judge/

# Assessing Nodel Answers through a LLM-as-a-judge (here, "gpt-4-0613" is used)
python gen_judgment.py --model-list <model_id>
		

# Giving a penalty to the score of non-Korean responses
cd data/mt_bench/model_judgment
python detector.py --model_id <model_id>
		
```

### 3. Show Result

```bash
cd fastchat/llm_judge/

# Getting Results
python show_result.py --mode single --input-file <file_path>

```

- Please put `./data/mt_bench/model_judgment/<model_id>_single_final.jsonl` into `<file_path>` to obtain the language-penalized results (default).
- If you want to see the unpenalized results, put `./data/mt_bench/model_judgment/<model_id>_single.jsonl` into `<file_path>` instead.

<br>

## Results

Here are the evaluation results of various language models including the EXAONE 3.0 7.8B instruction-tuned model on KoMT-Bench. Please refer to [EXAONE 3.0 technical report](https://arxiv.org/abs/2408.03541) for details.

| | EXAONE 3.0 7.8B Inst. | Llama 3.1 8B Inst. | Gemma 2 9B Inst. | QWEN 2 7B Inst. | Phi 3 7B Inst. | Mistral 7B Inst. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| KoMT-Bench | **8.92**  | 6.06 | 7.92 | 7.69 | 4.87 | 5.20 |

<br>

## Notice

### Why penalize scores for non-Korean response?

We found that, even when generated responses were in a language other than Korean, GPT-4-0613, acting as the judge, continued to award high scores. To handle such cases, we adopt a *square root penalty* which applies the square root to the score of non-Korean responses in order to adjust for this discrepancy.

By applying the square root penalty, the range of score for non-Korean responses falls within $[1,\sqrt{10}]$. It's worth noting that we do not apply this penalty to questions 138 and 140, as their potential responses could be non-Korean.

<br>

## References

[1] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 46595â€“46623. Curran Associates, Inc., 2023.

<br>

## Citation

```
@misc{KoMT-Bench,
  author = {LG AI Research},
  title = {KoMT-Bench},
  year = {2024},
  publisher = {Hugging Face},
  journal = {Hugging Face repository},
  howpublished = {\url{https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench}}
}
```
